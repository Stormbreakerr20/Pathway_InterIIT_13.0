$sources:
  - !pw.io.fs.read
    path: data
    format: binary
    with_metadata: true

$llm: !pw.xpacks.llm.llms.OpenAIChat
  model: "gpt-3.5-turbo"
  retry_strategy: !pw.udfs.ExponentialBackoffRetryStrategy
    max_retries: 6
  cache_strategy: !pw.udfs.DefaultCache
  temperature: 0
  capacity: 8

$embedder: !pw.xpacks.llm.embedders.SentenceTransformerEmbedder
  model: "sentence-transformers/all-MiniLM-L6-v2"
  # cache_strategy: !pw.udfs.DiskCache

$splitter: !langchain.text_splitter.RecursiveCharacterTextSplitter
  chunk_size: 1000
  chunk_overlap: 200


$parser: !pw.xpacks.llm.parsers.ParseUnstructured

$retriever_factory: !pw.stdlib.indexing.BruteForceKnnFactory
  reserved_space: 1000
  embedder: $embedder
  metric: !pw.stdlib.indexing.BruteForceKnnMetricKind.COS
  dimensions: 1536
  
$document_store: !pw.xpacks.llm.document_store.DocumentStore.from_langchain_components
  docs: $sources
  parser: $parser
  splitter: $splitter
  retriever_factory: $retriever_factory

question_answerer: !pw.xpacks.llm.question_answering.AdaptiveRAGQuestionAnswerer
  llm: $llm
  indexer: $document_store

# Change host and port by uncommenting these lines
host: "0.0.0.0"
port: 8000

# Cache configuration
with_cache: true

terminate_on_error: true