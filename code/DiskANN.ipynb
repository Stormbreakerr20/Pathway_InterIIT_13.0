{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjiTF35pubiO",
        "outputId": "83026593-c86b-4dd7-cef8-698bcfe81e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.27.0 requires sqlglot<25.2,>=23.6.3, but you have sqlglot 10.6.1 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "ibis-framework 9.2.0 requires sqlglot<25.7,>=23.4, but you have sqlglot 10.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-community pathway pyngrok sentence_transformers unstructured unstructured[pdf] pdfplumber pymupdf Pillow pandas tiktoken llm-app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtsV3c7Du8ki",
        "outputId": "41fbc95b-6273-4d05-abc9-7d80223c1bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD7XYsaBuhRz",
        "outputId": "c86d8d5c-e729-444d-adb5-18bdd84bec1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "! ngrok config add-authtoken 2oyvad5WHw2MTL0nUyx97i8M0ts_7WoRD7szU4fegHLhfEUSB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ySrm3gHY-dok"
      },
      "outputs": [],
      "source": [
        "# Copyright © 2024 Pathway\n",
        "\n",
        "import asyncio\n",
        "import functools\n",
        "import inspect\n",
        "import threading\n",
        "from collections.abc import Callable\n",
        "from typing import Any\n",
        "import pathway as pw\n",
        "import pathway.io as io\n",
        "\n",
        "class _RunThread(threading.Thread):\n",
        "    def __init__(self, coroutine):\n",
        "        self.coroutine = coroutine\n",
        "        self.result = None\n",
        "        super().__init__()\n",
        "\n",
        "    def run(self):\n",
        "        self.result = asyncio.run(self.coroutine)\n",
        "\n",
        "\n",
        "def _run_async(coroutine):\n",
        "    try:\n",
        "        loop = asyncio.get_running_loop()\n",
        "    except RuntimeError:\n",
        "        loop = None\n",
        "    if loop and loop.is_running():\n",
        "        thread = _RunThread(coroutine)\n",
        "        thread.start()\n",
        "        thread.join()\n",
        "        return thread.result\n",
        "    else:\n",
        "        return asyncio.run(coroutine)\n",
        "\n",
        "\n",
        "def _coerce_sync(func: Callable) -> Callable:\n",
        "    if asyncio.iscoroutinefunction(func):\n",
        "\n",
        "        @functools.wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            return _run_async(func(*args, **kwargs))\n",
        "\n",
        "        return wrapper\n",
        "    else:\n",
        "        return func\n",
        "\n",
        "\n",
        "def _check_model_accepts_arg(model_name: str, provider: str, arg: str):\n",
        "    from litellm import get_supported_openai_params\n",
        "\n",
        "    supported_params = (\n",
        "        get_supported_openai_params(model=model_name, custom_llm_provider=provider)\n",
        "        or []\n",
        "    )\n",
        "\n",
        "    return arg in supported_params\n",
        "\n",
        "\n",
        "def _extract_value(data: Any | pw.Json) -> Any:\n",
        "    if isinstance(data, pw.Json):\n",
        "        return data.value\n",
        "    return data\n",
        "\n",
        "\n",
        "def _unwrap_udf(func: pw.UDF | Callable) -> Callable:\n",
        "    \"\"\"Turn a Pathway UDF function into regular callable function.\"\"\"\n",
        "    if isinstance(func, pw.UDF):\n",
        "        return func.func  # use settings applied to a UDF\n",
        "    return func\n",
        "\n",
        "\n",
        "def get_func_arg_names(func):\n",
        "    sig = inspect.signature(func)\n",
        "    return [param.name for param in sig.parameters.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xRZESYwCKNzS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import pickle\n",
        "import os\n",
        "import pathway as pw\n",
        "from pathway.xpacks.llm.vector_store import VectorStoreServer\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cosine\n",
        "import faiss\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "@dataclass\n",
        "class Document:\n",
        "    id: str\n",
        "    content: str\n",
        "    metadata: Optional[Dict] = None\n",
        "\n",
        "@dataclass\n",
        "class Chunk:\n",
        "    id: str\n",
        "    doc_id: str\n",
        "    content: str\n",
        "    vector: np.ndarray = None\n",
        "    metadata: Optional[Dict] = None\n",
        "\n",
        "@dataclass\n",
        "class Cluster:\n",
        "    id: int\n",
        "    center: np.ndarray\n",
        "    chunks: List[Chunk]\n",
        "    quantized_center: np.ndarray = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EflyM1qUKefH"
      },
      "outputs": [],
      "source": [
        "# from pathway import Table\n",
        "from pathway.internals.datasink import DataSink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2UiEMHscMeA8"
      },
      "outputs": [],
      "source": [
        "# Implement a custom DataSink to collect data into a list\n",
        "class ListDataSink(DataSink):\n",
        "    def __init__(self):\n",
        "        self.data = []\n",
        "\n",
        "    def push_batch(self, batch):\n",
        "        self.data.extend(batch)\n",
        "\n",
        "    def finalize(self):\n",
        "        pass  # No action needed for list collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "Ld126LstujHm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import pickle\n",
        "import os\n",
        "import pathway as pw\n",
        "from pathway.xpacks.llm.vector_store import VectorStoreServer\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cosine\n",
        "import faiss\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "@dataclass\n",
        "class Document:\n",
        "    id: str\n",
        "    content: str\n",
        "    metadata: Optional[Dict] = None\n",
        "\n",
        "@dataclass\n",
        "class Chunk:\n",
        "    id: str\n",
        "    doc_id: str\n",
        "    content: str\n",
        "    vector: np.ndarray = None\n",
        "    metadata: Optional[Dict] = None\n",
        "\n",
        "@dataclass\n",
        "class Cluster:\n",
        "    id: int\n",
        "    center: np.ndarray\n",
        "    chunks: List[Chunk]\n",
        "    quantized_center: np.ndarray = None\n",
        "\n",
        "class EnhancedVectorStore:\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_model: str = \"nomic-ai/nomic-embed-text-v1\",\n",
        "        n_clusters: int = 10,\n",
        "        chunk_size: int = 500,\n",
        "        host: str = \"127.0.0.1\",\n",
        "        port: int = 8668,\n",
        "        base_path: str = \"vector_db\",\n",
        "        cache_dir: str = \"./Cache\",\n",
        "        license_key: str = None\n",
        "    ):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.n_clusters = n_clusters\n",
        "        self.chunk_size = chunk_size\n",
        "        self.host = host\n",
        "        self.port = port\n",
        "        self.base_path = base_path\n",
        "        self.cache_dir = cache_dir\n",
        "\n",
        "        # Initialize components\n",
        "        if license_key:\n",
        "            pw.set_license_key(license_key)\n",
        "\n",
        "        self.embeddings = SentenceTransformerEmbeddings(\n",
        "            model_name=embedding_model,\n",
        "            model_kwargs={\"trust_remote_code\": True}\n",
        "        )\n",
        "        self.text_splitter = CharacterTextSplitter(chunk_size=chunk_size)\n",
        "        self.clusters = []\n",
        "\n",
        "        # Create necessary directories\n",
        "        os.makedirs(base_path, exist_ok=True)\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    class RetrieveQuerySchema(pw.Schema):\n",
        "        query: str\n",
        "        k: int\n",
        "\n",
        "    def temp(self, files: List[Path]):\n",
        "      \"\"\"\n",
        "      Args: Given a list of file paths, iterate through each document,\n",
        "      read and create document chunks\n",
        "      \"\"\"\n",
        "\n",
        "      assert len(files) > 0, \"Files provided cannot be empty\"\n",
        "\n",
        "\n",
        "\n",
        "    def process_documents(self, documents: List[Document]) -> List[Chunk]:\n",
        "        \"\"\"Process documents into chunks with embeddings.\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for doc in documents:\n",
        "            # Split text into chunks\n",
        "            texts = self.text_splitter.split_text(doc.content)\n",
        "\n",
        "            # Create chunks with embeddings\n",
        "            for i, text in enumerate(texts):\n",
        "                chunk = Chunk(\n",
        "                    id=f\"{doc.id}_chunk_{i}\",\n",
        "                    doc_id=doc.id,\n",
        "                    content=text,\n",
        "                    metadata=doc.metadata\n",
        "                )\n",
        "                # Get embedding vector\n",
        "                chunk.vector = self.embeddings.embed_query(text)\n",
        "                chunks.append(chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def create_clusters(self, chunks: List[Chunk]):\n",
        "        \"\"\"Create clusters from chunks using K-means.\"\"\"\n",
        "        vectors = np.array([chunk.vector for chunk in chunks])\n",
        "\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)\n",
        "        labels = kmeans.fit_predict(vectors)\n",
        "\n",
        "        self.clusters = []\n",
        "        for i in range(self.n_clusters):\n",
        "            cluster_chunks = [chunk for j, chunk in enumerate(chunks) if labels[j] == i]\n",
        "            cluster = Cluster(\n",
        "                id=i,\n",
        "                center=kmeans.cluster_centers_[i],\n",
        "                chunks=cluster_chunks,\n",
        "                quantized_center=self._quantize_vector(kmeans.cluster_centers_[i])\n",
        "            )\n",
        "            self.clusters.append(cluster)\n",
        "            self._store_cluster(cluster)\n",
        "\n",
        "    def _quantize_vector(self, vector: np.ndarray, bits: int = 8) -> np.ndarray:\n",
        "        \"\"\"Quantize vector to reduce memory footprint.\"\"\"\n",
        "        max_val = np.max(np.abs(vector))\n",
        "        scale = (2 ** (bits - 1) - 1) / max_val\n",
        "        quantized = np.round(vector * scale)\n",
        "        return quantized / scale\n",
        "\n",
        "    def _store_cluster(self, cluster: Cluster):\n",
        "        \"\"\"Store cluster data using FAISS.\"\"\"\n",
        "        index = faiss.IndexFlatL2(len(cluster.chunks[0].vector))\n",
        "        vectors = np.array([chunk.vector for chunk in cluster.chunks])\n",
        "        index.add(vectors)\n",
        "\n",
        "        cluster_path = os.path.join(self.base_path, f\"cluster_{cluster.id}\")\n",
        "        faiss.write_index(index, f\"{cluster_path}.index\")\n",
        "\n",
        "        chunk_metadata = {chunk.id: chunk for chunk in cluster.chunks}\n",
        "        with open(f\"{cluster_path}_metadata.pkl\", \"wb\") as f:\n",
        "            pickle.dump(chunk_metadata, f)\n",
        "\n",
        "    def start_server(self):\n",
        "\n",
        "        # Set up ngrok tunnel\n",
        "        public_url = ngrok.connect(self.port)\n",
        "        print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://{self.host}:{self.port}\\\"\")\n",
        "\n",
        "        # Run server\n",
        "        self.run_server()\n",
        "\n",
        "        return public_url\n",
        "\n",
        "    def similarity_search(self, retrieval_queries) -> List[Chunk]:\n",
        "        \"\"\"Perform two-stage similarity search.\"\"\"\n",
        "\n",
        "        query = str(retrieval_queries[\"query\"])\n",
        "        n_results = 5\n",
        "\n",
        "\n",
        "        query_vector = self.embeddings.embed_query(str(query))\n",
        "\n",
        "\n",
        "\n",
        "        # Stage 1: Find closest clusters\n",
        "        cluster_distances = []\n",
        "        for cluster in self.clusters:\n",
        "            distance = cosine(query_vector, cluster.quantized_center)\n",
        "            cluster_distances.append((cluster, distance))\n",
        "\n",
        "        top_clusters = sorted(cluster_distances, key=lambda x: x[1])[:3]\n",
        "\n",
        "        # Stage 2: Search within top clusters\n",
        "        results = []\n",
        "        for cluster, _ in top_clusters:\n",
        "            cluster_path = os.path.join(self.base_path, f\"cluster_{cluster.id}\")\n",
        "            index = faiss.read_index(f\"{cluster_path}.index\")\n",
        "\n",
        "            with open(f\"{cluster_path}_metadata.pkl\", \"rb\") as f:\n",
        "                chunk_metadata = pickle.load(f)\n",
        "\n",
        "            D, I = index.search(np.array([query_vector]), n_results)\n",
        "\n",
        "            for idx in I[0]:\n",
        "                chunk_id = list(chunk_metadata.keys())[idx]\n",
        "                results.append(chunk_metadata[chunk_id])\n",
        "\n",
        "        # Rerank results\n",
        "        return self._rerank_results(results, query_vector)[:n_results]\n",
        "\n",
        "    def _rerank_results(self, chunks: List[Chunk], query_vector: np.ndarray) -> List[Chunk]:\n",
        "        \"\"\"Rerank results based on cosine similarity.\"\"\"\n",
        "        chunk_scores = []\n",
        "        for chunk in chunks:\n",
        "            similarity = 1 - cosine(query_vector, chunk.vector)\n",
        "            chunk_scores.append((chunk, similarity))\n",
        "\n",
        "        reranked = sorted(chunk_scores, key=lambda x: x[1], reverse=True)\n",
        "        return [chunk for chunk, _ in reranked]\n",
        "\n",
        "    def run_server(\n",
        "        self,\n",
        "        threaded=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Builds the document processing pipeline and runs it without pw.run\n",
        "\n",
        "        Args:\n",
        "            threaded: if True, run in a thread. Else block computation\n",
        "            kwargs: optional parameters (unused)\n",
        "        \"\"\"\n",
        "        webserver = pw.io.http.PathwayWebserver(host=self.host, port=self.port, with_cors=True)\n",
        "\n",
        "        # Helper function to set up routes\n",
        "        def serve(route, schema, handler, documentation):\n",
        "            queries, writer = pw.io.http.rest_connector(\n",
        "                webserver=webserver,\n",
        "                route=route,\n",
        "                methods=(\"GET\", \"POST\"),\n",
        "                schema=schema,\n",
        "                autocommit_duration_ms=50,\n",
        "                delete_completed_queries=False,\n",
        "                documentation=documentation,\n",
        "            )\n",
        "            writer(handler(queries))\n",
        "\n",
        "        # Set up the retrieve endpoint\n",
        "        serve(\n",
        "            \"/v1/retrieve\",\n",
        "            self.RetrieveQuerySchema,\n",
        "            self.similarity_search,\n",
        "            pw.io.http.EndpointDocumentation(\n",
        "                summary=\"Do a similarity search for your query\",\n",
        "                description=\"Request the given number of documents from the \"\n",
        "                \"realtime-maintained index.\",\n",
        "                method_types=(\"GET\",),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Run the server\n",
        "        if threaded:\n",
        "            import threading\n",
        "            thread = threading.Thread(target=webserver.run)\n",
        "            thread.daemon = True  # Allow the thread to be terminated when the main program exits\n",
        "            thread.start()\n",
        "            return thread\n",
        "        else:\n",
        "            webserver._run()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "9LPcB--_vvh0"
      },
      "outputs": [],
      "source": [
        "pw.set_license_key(\"4AB076-24E87E-EA755E-6ED4AD-6AFAAC-V3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "NkCYNkrGQpD8"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import fitz  # PyMuPDF\n",
        "import io\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Function to extract tables using pdfplumber\n",
        "def extract_tables(pdf_bytes):\n",
        "    tables = []\n",
        "    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            extracted_tables = page.extract_tables()\n",
        "            for table in extracted_tables:\n",
        "                tables.append(table)\n",
        "    return tables\n",
        "\n",
        "\n",
        "# Function to extract images using PyMuPDF and save them\n",
        "def extract_images(pdf_bytes, pdf_path):\n",
        "    images_info = []\n",
        "    with fitz.open(stream=pdf_bytes, filetype=\"pdf\") as doc:\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            image_list = page.get_images(full=True)\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                xref = img[0]\n",
        "                base_image = doc.extract_image(xref)\n",
        "                image_bytes = base_image[\"image\"]\n",
        "                image_ext = base_image[\"ext\"]\n",
        "                # Save image to filesystem\n",
        "                image_filename = f\"{Path(pdf_path).stem}_page{page_num + 1}_img{img_index + 1}.{image_ext}\"\n",
        "                image_path = os.path.join(\"extracted_images\", image_filename)\n",
        "                os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
        "                with open(image_path, \"wb\") as img_file:\n",
        "                    img_file.write(image_bytes)\n",
        "                images_info.append({\n",
        "                    \"page\": page_num + 1,\n",
        "                    \"image_index\": img_index + 1,\n",
        "                    \"image_path\": image_path,\n",
        "                    \"image_ext\": image_ext\n",
        "                })\n",
        "    return images_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFu82kCDu1i5",
        "outputId": "b6d7d741-b252-492e-dc83-6bb6c60c2369"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/eb02ceb48c1fdcc477ff1925c9732c379f0f0d1f/modeling_hf_nomic_bert.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = loader(resolved_archive_file)\n",
            "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.eb02ceb48c1fdcc477ff1925c9732c379f0f0d1f.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize enhanced vector store\n",
        "vector_store = EnhancedVectorStore(\n",
        "    embedding_model=\"nomic-ai/nomic-embed-text-v1\",\n",
        "    n_clusters=5,\n",
        "    chunk_size=500,\n",
        "    license_key=None\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# # Sample documents\n",
        "# documents = [\n",
        "#     Document(id=\"1\", content=gandhi, metadata={\"source\": \"file1.txt\"}),\n",
        "#     Document(id=\"2\", content=luther, metadata={\"source\": \"file2.txt\"})\n",
        "# ]\n",
        "\n",
        "data = pw.io.fs.read(\n",
        "    \"./data\",\n",
        "    format=\"binary\",\n",
        "    mode=\"static\",\n",
        "    with_metadata=True,\n",
        "    autocommit_duration_ms=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxdCi7e8hRaj",
        "outputId": "044500ef-36fd-496e-eed1-7d930dc68757"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pathway_engine.connectors.monitoring:PosixLikeReader-0: Closing the data source\n"
          ]
        }
      ],
      "source": [
        "# Convert the extracted_table to a Pandas DataFrame\n",
        "df = pw.debug.table_to_pandas(data)\n",
        "df = df.reset_index()\n",
        "df = df.drop([\"index\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TddrJ-wzldHD",
        "outputId": "d9695141-68a4-4fb2-f067-8ea7447a0619"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pathway_engine.connectors.monitoring:PosixLikeReader-0: Closing the data source\n"
          ]
        }
      ],
      "source": [
        "documents = data.select(texts=extract_texts(pw.this.data))\n",
        "df2 = pw.debug.table_to_pandas(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgT8Kskjhkev",
        "outputId": "af362826-fe78-4b75-e681-64a68a3eaa42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"created_at\": 1732995945, \"modified_at\": 1732995945, \"owner\": \"root\", \"path\": \"data/IYEP-BiswadeepPurkayastha.pdf\", \"seen_at\": 1733000400}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-174-33b994bc3aef>:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  text = copy.deepcopy(df2['texts'][index])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"created_at\": 1732989590, \"modified_at\": 1732989590, \"owner\": \"root\", \"path\": \"data/threads-bugs.pdf\", \"seen_at\": 1733000400}\n",
            "{\"created_at\": 1732996186, \"modified_at\": 1732996186, \"owner\": \"root\", \"path\": \"data/Lecture 14-15 - 12 Sep 24.pdf\", \"seen_at\": 1733000400}\n",
            "{\"created_at\": 1733000219, \"modified_at\": 1733000219, \"owner\": \"root\", \"path\": \"data/gandhi.pdf\", \"seen_at\": 1733000400}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from llm_app import extract_texts\n",
        "import copy\n",
        "# Process each row in df\n",
        "extracted_data = []\n",
        "for index, row in df.iterrows():\n",
        "    print(row['_metadata'])\n",
        "    pdf_path = row['_metadata']['path']\n",
        "    pdf_bytes = row['data']\n",
        "    # try:\n",
        "    text = copy.deepcopy(df2['texts'][index])\n",
        "    tables = extract_tables(pdf_bytes)\n",
        "    # images_info = extract_images(pdf_bytes, pdf_path)\n",
        "    extracted_data.append({\n",
        "        'path': pdf_path,\n",
        "        'text': text,\n",
        "        'tables': tables,\n",
        "        # 'images': images_info\n",
        "    })\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error processing {pdf_path}: {e}\")\n",
        "    #     extracted_data.append({\n",
        "    #         'path': pdf_path,\n",
        "    #         'text': \"\",\n",
        "    #         'tables': [],\n",
        "    #         'images': []\n",
        "    #     })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "q5dB01T9hsoZ"
      },
      "outputs": [],
      "source": [
        "# Convert extracted_data to pandas DataFrame\n",
        "extracted_df = pd.DataFrame(extracted_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "HUnXOr_rhufg"
      },
      "outputs": [],
      "source": [
        "# Serialize 'tables' and 'images' columns to JSON strings\n",
        "extracted_df['tables'] = extracted_df['tables'].apply(lambda x: json.dumps(x))\n",
        "# extracted_df['images'] = extracted_df['images'].apply(lambda x: json.dumps(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "7SojXNMqff5-"
      },
      "outputs": [],
      "source": [
        "# Save extracted_df to CSV\n",
        "extracted_df.to_csv('extracted_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "WvPgeJYmtDZ7"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "\n",
        "for index, row in extracted_df.iterrows():\n",
        "    text = row['text']\n",
        "    paragraph = '\\n\\n'.join(text)\n",
        "    doc = Document(id=str(index+1), content=paragraph, metadata={\"source\": row['path']})\n",
        "    documents.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbHsWViKX2Vi",
        "outputId": "353fb6fe-4bc3-4819-f29f-575cdabe1d62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 628, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 538, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 561, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 590, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 527, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 793, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 510, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 766, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1117, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 537, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 994, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 509, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 646, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 821, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1148, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 569, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 640, which is longer than the specified 500\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Process documents and create clusters\n",
        "chunks = vector_store.process_documents(documents)\n",
        "vector_store.create_clusters(chunks)\n",
        "\n",
        "# Start server\n",
        "# public_url = vector_store.start_server()\n",
        "# print(f\"Server running at {public_url}\")\n",
        "\n",
        "# # Example search\n",
        "# results = vector_store.similarity_search(\"sample query\", n_results=5)\n",
        "# for chunk in results:\n",
        "#     print(f\"Document: {chunk.doc_id}, Chunk: {chunk.id}, Content: {chunk.content[:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_7YMzeDwq7X",
        "outputId": "44aa17c0-ce48-441f-e12e-82310d84b1be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "129"
            ]
          },
          "execution_count": 180,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKOJzWXq2alB",
        "outputId": "fc1d517b-2ea9-4b8f-e1a6-233e435883d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document: 3, Chunk: 3_chunk_4, Content: Gandhi's birthday, 2 October, is commemorated in India as Gandhi Jayanti, a national holiday, and wo...\n",
            "Document: 3, Chunk: 3_chunk_3, Content: Gandhi's vision of an independent India based on religious pluralism was challenged in the early 194...\n",
            "Document: 3, Chunk: 3_chunk_0, Content: Mohandas Karamchand Gandhi (ISO: Mōhanadāsa Karamacaṁda Gāṁdhī;[c] 2 October 1869 – 30 January 1948)...\n",
            "Document: 3, Chunk: 3_chunk_2, Content: Assuming leadership of the Indian National Congress in 1921, Gandhi led nationwide campaigns for eas...\n",
            "Document: 3, Chunk: 3_chunk_1, Content: Born and raised in a Hindu family in coastal Gujarat, Gandhi trained in the law at the Inner Temple ...\n"
          ]
        }
      ],
      "source": [
        "# Example search\n",
        "results = vector_store.similarity_search({\"query\": \"When is Gandhi Jayanti celebrated\"})\n",
        "for chunk in results:\n",
        "    print(f\"Document: {chunk.doc_id}, Chunk: {chunk.id}, Content: {chunk.content[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "rdDqBX6W1gmy"
      },
      "outputs": [],
      "source": [
        "!killall ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl6z9agbCaWS"
      },
      "outputs": [],
      "source": [
        "# Example search\n",
        "results = vector_store.similarity_search({\"query\": \"When is Gandhi Jayanti celebrated\"})\n",
        "for chunk in results:\n",
        "    print(f\"Document: {chunk.doc_id}, Chunk: {chunk.id}, Content: {chunk.content[:100]}...\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
